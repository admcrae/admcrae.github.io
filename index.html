<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>A McRae | Home</title>
    <link rel="icon" type="image/png" href="/files/favicon.png">
  </head>
  <body>
    <nav>
      <h3>Navigation</h4>
      <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/publications/">Publications</a></li>
        <li><a href="/teaching/">Teaching</a></li>
        <li><a href="/travel/">Travel</a></li>
      </ul>
    </nav>
    <section>
      <h2>Andrew D. McRae</h2>
<ul>
    <li>E-mail: <a href="mailto:andrew.mcrae@epfl.ch">andrew.mcrae@epfl.ch</a></li>
    <li><a href="files/mcrae_cv.pdf">CV</a></li>
    <li><a href="https://people.epfl.ch/andrew.mcrae/?lang=en">EPFL directory page</a> with more contact information</li>
</ul>

<main>
    <p>
    I am a postdoc in the <a href="https://www.epfl.ch/labs/optim/">OPTIM</a> lab at EPFL, working with <a href="https://sma.epfl.ch/~nboumal/">Nicolas Boumal</a>.
    </p>
    <p>
    I graduated from Georgia Tech in May 2022 with a Ph.D. in <a href="https://ece.gatech.edu">Electrical and Computer Engineering</a>,
    advised by <a href="https://mdav.ece.gatech.edu">Mark Davenport</a>.
    </p>
    <p>
    My doctoral research was in learning theory and high-dimensional statistics.
    My postdoc work has shifted focus toward the specific optimization problems that arise from such problems.
    Most of my work falls under the broad umbrella of understanding how <i>problem structure</i> affects
    <ol type=a>
        <li>how many measurements/samples we need to make useful predictions or inferences,</li>
        <li>how much error/corruption we can expect due to noise or other factors, and</li>
        <li>how difficult the associated optimization problems are to solve.</li>
    </ol>
    </p>
    <p>
    Here is a non-comprehensive list of topics I've worked on recently:
    <ul>
        <li>Nonconvex optimization landscapes arising from statistics and machine learning problems</li>
        <li>Low-rank (noisy) matrix completion via convex optimization</li>
        <li>Convex optimization for nonlinear recovery via lifting
            <ul>
                <li>Reproducing kernel Hilbert space (RKHS) methods</li>
                <li>(Sparse) phase retrieval</li>
                <li>(Sparse) principal component analysis (PCA)</li>
            </ul>
        </li>
        <li>Interpolation with noise (a.k.a. "Benign overfitting" or "Harmless interpolation")</li>
        <li>Classification theory (and how it differs from regression)</li>
        <li>Regression on a manifold domain</li>
    </ul>
    For publications, see my <a href="/publications/">publications page</a> or <a href="https://scholar.google.com/citations?user=GT_Ml_cAAAAJ">Google Scholar profile</a>.
    </p>

</main>

    </section>
  </body>
</html>